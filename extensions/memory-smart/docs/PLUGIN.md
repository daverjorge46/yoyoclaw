# memory-smart â€” Smart AI-Powered Memory for OpenClaw

## Overview

`memory-smart` is a Clawdbot/OpenClaw plugin that provides AI-powered long-term memory with provider-agnostic embeddings, entity awareness, and automatic fact extraction.

It implements a **three-tier memory model**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TIER 1: CORE                      â”‚
â”‚         Always in context (~400 tokens)              â”‚
â”‚   Identity, active rules, current project state      â”‚
â”‚         Self-edited by agent via tool call            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                TIER 2: ACTIVE INDEX                   â”‚
â”‚         LanceDB vector store (~0 tokens at rest)     â”‚
â”‚   Structured facts, entity profiles, preferences     â”‚
â”‚   Retrieved on-demand via semantic search             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              TIER 3: ARCHIVE (existing)              â”‚
â”‚         Markdown files + session transcripts         â”‚
â”‚   Raw daily logs, MEMORY.md, session JSONLs          â”‚
â”‚   Searched via existing memory_search/memory_get     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Data flows DOWN:** Raw conversations â†’ AI extraction â†’ structured facts in LanceDB â†’ important things surface to core memory.

**Data flows UP:** Core memory is always visible. Active index is searched on-demand. Archive is a fallback.

### Why memory-smart?

Current OpenClaw memory options are limited:
- **memory-core** (default): Markdown search via embeddings. No auto-capture, no auto-recall injection, no fact extraction.
- **memory-lancedb**: Has auto-recall/capture but locked to OpenAI embeddings. Capture is regex-based (dumb). No entity awareness.

**memory-smart** provides:
- âœ… Provider-agnostic embeddings (Gemini, OpenAI)
- âœ… AI-powered fact extraction (uses Gemini Flash for structured extraction)
- âœ… Core memory block (small, always-in-context, self-edited)
- âœ… Entity-aware retrieval (structured knowledge about people/projects)
- âœ… Memory consolidation (dedup, compress, decay, prune)
- âœ… Background reflection (periodic review and cleanup)

---

## Quick Start

### 1. Enable the plugin

In your Clawdbot config:

```json5
{
  plugins: {
    slots: {
      memory: "memory-smart"
    },
    entries: {
      "memory-smart": {
        enabled: true,
        config: {
          // Minimal config â€” auto-detects Gemini or OpenAI from env vars
        }
      }
    }
  }
}
```

### 2. Set API key

Set `GEMINI_API_KEY` in your environment. Gemini embeddings are free with generous quotas.

```bash
export GEMINI_API_KEY="your-key-here"
```

### 3. Import existing memories (optional)

If you're migrating from `memory-core`:

```bash
openclaw smart-memory import --source workspace
```

This reads your `MEMORY.md` and `memory/*.md` files, extracts facts via AI, and populates the vector store.

### 4. Verify

```bash
openclaw smart-memory stats
```

---

## Configuration Reference

```json5
{
  "memory-smart": {
    config: {
      // â”€â”€â”€ Embedding Provider â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      embedding: {
        provider: "auto",              // "gemini" | "openai" | "auto"
                                       // auto: Gemini if GEMINI_API_KEY set, else OpenAI
        apiKey: "${GEMINI_API_KEY}",   // API key (supports ${ENV_VAR} syntax)
        model: "gemini-embedding-001"  // Embedding model
                                       // Gemini: gemini-embedding-001 (3072d, free)
                                       // OpenAI: text-embedding-3-small (1536d, $0.02/M)
                                       //         text-embedding-3-large (3072d, $0.13/M)
      },

      // â”€â”€â”€ AI Extraction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      extraction: {
        enabled: true,                 // Enable AI fact extraction
        provider: "gemini",            // LLM for extraction
        model: "gemini-2.5-flash",     // Cheap + fast model
        apiKey: "${GEMINI_API_KEY}",   // Falls back to embedding.apiKey
        maxFactsPerConversation: 10,   // Max facts per conversation
        minConversationLength: 3       // Min messages to trigger extraction
      },

      // â”€â”€â”€ Core Memory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      coreMemory: {
        enabled: true,                 // Enable core memory block
        maxTokens: 1500,               // Token budget (~4 chars/token)
        filePath: "memory/core.md"     // Path relative to workspace
      },

      // â”€â”€â”€ Entity Profiles â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      entities: {
        enabled: true,                 // Enable entity tracking
        autoCreate: true,              // Auto-create entities from extractions
        minMentionsToCreate: 3         // Min mentions before auto-creating
      },

      // â”€â”€â”€ Auto-Recall (Context Injection) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      autoRecall: {
        enabled: true,                 // Inject memories before each agent run
        maxResults: 5,                 // Max memories to inject
        maxTokens: 2000,               // Total injection token budget
        minScore: 0.3,                 // Min similarity score (0-1)
        entityBoost: true              // Boost results for mentioned entities
      },

      // â”€â”€â”€ Auto-Capture â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      autoCapture: {
        enabled: true                  // Queue conversations for extraction
      },

      // â”€â”€â”€ Reflection (Background Maintenance) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      reflection: {
        enabled: true,                 // Enable reflection pipeline
        intervalMinutes: 360,          // Run every 6 hours
        maxOperationsPerRun: 50,       // Max operations per run
        deduplicateThreshold: 0.92,    // Cosine similarity for dedup
        decayDays: 90,                 // Days before memory importance decays
        pruneThreshold: 0.1            // Min importance before pruning
      },

      // â”€â”€â”€ Storage â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      store: {
        dbPath: "~/.openclaw/memory/smart-memory"  // LanceDB directory
      }
    }
  }
}
```

---

## Tools

### `memory_recall`

Semantic search through the fact store.

**Parameters:**
- `query` (string, required): Search query
- `limit` (number, optional): Max results (default 5)

**Example:**
```
memory_recall({ query: "EverWhen marketing strategy", limit: 3 })
```

**Returns:** Array of matching facts with relevance scores.

### `memory_store`

Manually store a fact in long-term memory.

**Parameters:**
- `text` (string, required): The fact to remember
- `category` (string, optional): preference | decision | fact | entity | rule | project | relationship | other
- `importance` (number, optional): 0.0-1.0 (default 0.5)

**Example:**
```
memory_store({
  text: "Jack prefers concise Telegram messages over long explanations",
  category: "preference",
  importance: 0.9
})
```

### `memory_forget`

Delete a specific memory by ID or by text search.

**Parameters:**
- `id` (string, optional): Exact memory ID to delete
- `query` (string, optional): Text to search and delete matching memories

### `core_memory_update`

Edit the always-in-context core memory block.

**Parameters:**
- `section` (string, required): "identity" | "human" | "rules" | "active_context" | "relationships"
- `content` (string, required): New content for the section
- `mode` (string, optional): "replace" | "append" | "remove_line" (default "replace")

**Example:**
```
core_memory_update({
  section: "active_context",
  content: "- Currently building memory-smart plugin for OpenClaw",
  mode: "append"
})
```

### `entity_lookup`

Look up everything known about a person, project, or entity.

**Parameters:**
- `name` (string, required): Entity name or alias

**Returns:** Entity profile with summary, type, aliases, and all linked facts.

---

## How It Works

### Auto-Recall (Context Injection)

On every `before_agent_start` event:

1. **Core memory** is always injected (~400 tokens)
2. The user's message is embedded and searched against the fact store
3. If known entity names appear in the message, their profiles are included
4. Results are injected as XML blocks:

```xml
<core-memory>
[core memory block content]
</core-memory>

<relevant-memories>
1. [preference] Jack prefers action over questions. (95%)
2. [rule] Never code on EverWhen repos â€” marketing only. (93%)
3. [project] Mission Control deployed at mission-control-ten-ochre.vercel.app (87%)
</relevant-memories>
```

Total injection budget: 2,000 tokens max.

### Auto-Capture

On every `agent_end` event:

1. User + assistant messages are extracted from the conversation
2. Very short exchanges (<3 messages) are skipped
3. Memory injection blocks are filtered out
4. The conversation is queued for batch AI extraction

### AI Fact Extraction

During the reflection pipeline (or import):

1. Queued conversations are sent to Gemini Flash
2. The AI extracts structured facts: `{ text, category, importance, entities }`
3. Each fact is self-contained (understandable without context)
4. Max 10 facts per conversation

**Extraction cost:** ~$0.001-0.003/day at 25 conversations/day.

### Reflection Pipeline

Runs every 6 hours (configurable) or manually via `smart-memory reflect`:

1. **Extract:** Process queued conversations â†’ new facts
2. **Store:** Embed and store facts (with dedup check)
3. **Link:** Match facts to entities â†’ update entity profiles
4. **Consolidate:** Merge near-duplicate memories (>0.92 similarity)
5. **Decay:** Reduce importance of stale, unaccessed memories (>90 days)
6. **Promote:** Surface high-importance facts to core memory "Active Context"

---

## Migration from memory-core

### Step-by-Step

1. **Configure the plugin** in your Clawdbot config (see Quick Start above)

2. **Set your API key:**
   ```bash
   export GEMINI_API_KEY="your-key"
   ```

3. **Import existing memories:**
   ```bash
   openclaw smart-memory import --source workspace
   ```
   This reads:
   - `MEMORY.md` (long-term curated memories)
   - `memory/*.md` (daily logs)
   - `SOUL.md`, `USER.md`, `IDENTITY.md` (for core memory generation)

4. **Verify the import:**
   ```bash
   openclaw smart-memory stats
   openclaw smart-memory search "some topic you remember"
   ```

5. **Your existing files are preserved.** The plugin adds a layer ON TOP â€” it doesn't replace or modify your markdown files.

6. **Fallback:** If memory-smart fails, the existing `memory_search`/`memory_get` tools from memory-core still work.

---

## CLI Reference

### `smart-memory stats`

Show comprehensive statistics: memory counts by category, entity counts by type, core memory usage, queue size, reflection history, config info, and disk usage.

```bash
openclaw smart-memory stats
```

### `smart-memory search <query>`

Semantic search through memories.

```bash
openclaw smart-memory search "EverWhen marketing"
openclaw smart-memory search "Jack's preferences" --limit 20
openclaw smart-memory search "deployment" --category project --min-score 0.5
```

**Options:**
- `--limit <n>` â€” Max results (default 10)
- `--category <cat>` â€” Filter by category
- `--min-score <n>` â€” Min similarity score 0-1 (default 0.3)

### `smart-memory entities`

List all entity profiles.

```bash
openclaw smart-memory entities
openclaw smart-memory entities --type person
openclaw smart-memory entities --sort recent
```

**Options:**
- `--type <type>` â€” Filter: person|project|tool|place|organization
- `--sort <field>` â€” Sort by: mentions|recent|name (default: mentions)

### `smart-memory entity <name>`

Show detailed entity profile with summary, aliases, and linked facts.

```bash
openclaw smart-memory entity "Jack"
openclaw smart-memory entity "EverWhen"
```

### `smart-memory core`

Display current core memory contents with per-section token counts and a usage bar.

```bash
openclaw smart-memory core
```

### `smart-memory reflect`

Manually trigger the full reflection pipeline (extraction, dedup, consolidation, decay, promotion).

```bash
openclaw smart-memory reflect
```

### `smart-memory export`

Export all memories and entities.

```bash
openclaw smart-memory export > memories.json
openclaw smart-memory export --format markdown > memories.md
```

**Options:**
- `--format <fmt>` â€” Output format: json|markdown (default: json)

### `smart-memory import`

Import from existing workspace memory files.

```bash
openclaw smart-memory import --source workspace
```

**Options:**
- `--source <source>` â€” Import source (currently only "workspace")

### `smart-memory reset`

Reset the database (delete all memories and entities). Core memory file is NOT deleted.

```bash
openclaw smart-memory reset --force
```

**Options:**
- `--force` â€” Skip confirmation prompt

---

## Architecture

### Component Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        memory-smart plugin                        â”‚
â”‚                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Providers  â”‚  â”‚   Stores   â”‚  â”‚      Lifecycle Hooks      â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚ Gemini     â”‚  â”‚ MemoryDB   â”‚  â”‚ auto-recall              â”‚   â”‚
â”‚  â”‚ OpenAI     â”‚  â”‚ EntityDB   â”‚  â”‚ (before_agent_start)     â”‚   â”‚
â”‚  â”‚ (factory)  â”‚  â”‚ CoreMemory â”‚  â”‚                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ auto-capture             â”‚   â”‚
â”‚                                   â”‚ (agent_end)              â”‚   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                          â”‚   â”‚
â”‚  â”‚   Tools    â”‚  â”‚ Extraction â”‚  â”‚ reflection               â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚ (background interval)    â”‚   â”‚
â”‚  â”‚ recall     â”‚  â”‚ extractor  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”‚ store      â”‚  â”‚ queue      â”‚                                    â”‚
â”‚  â”‚ forget     â”‚  â”‚ resolver   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ core_upd   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚          CLI              â”‚   â”‚
â”‚  â”‚ entity_lkp â”‚                   â”‚  stats, search, reflect  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚  export, import, reset   â”‚   â”‚
â”‚                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                   â”‚
          â–¼                   â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  LanceDB   â”‚     â”‚  Gemini Flash  â”‚
   â”‚  (vectors) â”‚     â”‚  (extraction)  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
User Message
     â”‚
     â–¼
[before_agent_start]  â”€â”€â†’  embed query â”€â”€â†’ search LanceDB â”€â”€â†’ inject context
     â”‚
     â–¼
[agent runs with injected memories]
     â”‚
     â–¼
[agent_end]  â”€â”€â†’  queue conversation for extraction
     â”‚
     â–¼
[reflection job runs every 6h]
     â”‚
     â”œâ”€â”€â†’ Extract: AI extraction via Gemini Flash
     â”œâ”€â”€â†’ Store: embed + store facts (dedup check)
     â”œâ”€â”€â†’ Link: resolve entities, update profiles
     â”œâ”€â”€â†’ Consolidate: merge near-duplicates
     â”œâ”€â”€â†’ Decay: reduce importance of stale memories
     â””â”€â”€â†’ Promote: surface important facts to core memory
```

### File Structure

```
memory-smart/
â”œâ”€â”€ openclaw.plugin.json          # Plugin manifest
â”œâ”€â”€ package.json                  # Dependencies
â”œâ”€â”€ index.ts                      # Plugin entry point
â”œâ”€â”€ config.ts                     # Config schema + validation
â”œâ”€â”€ providers/
â”‚   â”œâ”€â”€ types.ts                  # EmbeddingProvider interface
â”‚   â”œâ”€â”€ factory.ts                # Provider factory
â”‚   â”œâ”€â”€ gemini.ts                 # Gemini embeddings
â”‚   â””â”€â”€ openai.ts                 # OpenAI embeddings
â”œâ”€â”€ store/
â”‚   â”œâ”€â”€ memory-db.ts              # LanceDB memory store (facts)
â”‚   â”œâ”€â”€ entity-db.ts              # LanceDB entity store
â”‚   â””â”€â”€ core-memory.ts            # Core memory file manager
â”œâ”€â”€ extraction/
â”‚   â”œâ”€â”€ extractor.ts              # AI fact extraction (Gemini Flash)
â”‚   â”œâ”€â”€ entity-resolver.ts        # Entity name resolution + linking
â”‚   â””â”€â”€ queue.ts                  # Extraction queue (batch processing)
â”œâ”€â”€ lifecycle/
â”‚   â”œâ”€â”€ auto-recall.ts            # before_agent_start hook
â”‚   â”œâ”€â”€ auto-capture.ts           # agent_end hook
â”‚   â””â”€â”€ reflection.ts             # Background reflection job
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ memory-recall.ts          # memory_recall tool
â”‚   â”œâ”€â”€ memory-store.ts           # memory_store tool
â”‚   â”œâ”€â”€ memory-forget.ts          # memory_forget tool
â”‚   â”œâ”€â”€ core-memory-update.ts     # core_memory_update tool
â”‚   â””â”€â”€ entity-lookup.ts          # entity_lookup tool
â”œâ”€â”€ cli/
â”‚   â”œâ”€â”€ commands.ts               # CLI command registration
â”‚   â””â”€â”€ import.ts                 # Workspace import tool
â””â”€â”€ docs/
    â””â”€â”€ PLUGIN.md                 # This documentation
```

---

## FAQ

### What API keys do I need?

At minimum, one of:
- `GEMINI_API_KEY` â€” Free Gemini embeddings + cheap Flash extraction (~$0.003/day)
- `OPENAI_API_KEY` â€” OpenAI embeddings ($0.02-0.13/M tokens)

Gemini is recommended: free embeddings, generous quotas, and Flash is very cheap for extraction.

### How much disk space does it use?

Typically under 50MB even with thousands of facts. LanceDB stores vectors efficiently (3072-dim Ã— 4 bytes Ã— 1000 facts â‰ˆ 12MB plus metadata).

### Can I switch embedding providers?

Changing providers requires re-embedding all vectors. Use `smart-memory reset` and then `smart-memory import` to re-import from your workspace files.

### Does this replace my MEMORY.md and daily logs?

No. The plugin adds a layer ON TOP of your existing markdown files. It reads from them during import but never modifies them. Your existing `memory_search` and `memory_get` tools still work as fallback.

### How many tokens does auto-recall add per session?

Core memory: ~400 tokens (always). Relevant memories: 0-1,000 tokens (only if matches found). Total: ~400-1,400 tokens. Much less than manually reading MEMORY.md + daily logs (~5,000+ tokens).

### What happens if the Gemini API is down?

- Auto-recall: will warn and skip (no context injection, but agent still works)
- Auto-capture: conversations are queued and extracted later
- Reflection: scheduled run retries on next interval
- Tools: will return error messages to the agent

### Can I use this with multiple agents?

Yes, each agent gets its own LanceDB path via `store.dbPath`. Core memory files are also per-workspace.

---

*Plugin by Buddy ğŸ• | Built for OpenClaw/Clawdbot*
